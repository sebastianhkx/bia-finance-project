{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentimentAnalyzer(TickerName):\n",
    "    from bs4 import BeautifulSoup\n",
    "    from pprint import pprint\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    url = \"https://finviz.com/quote.ashx?t=\"+ str(TickerName)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    html_tables =[]\n",
    "    for data in soup:\n",
    "        data = soup.find(id='news-table')\n",
    "        html_tables.append(data)\n",
    "\n",
    "    # Hold the parsed news into a list\n",
    "    parsed_news = []\n",
    "    # Iterate through the news\n",
    "    for news_table in html_tables:\n",
    "        # Iterate through all tr tags in 'news_table'\n",
    "        for x in news_table.findAll('tr'):\n",
    "            # Read the text from the tr tag into text\n",
    "            text = x.get_text() \n",
    "            # Split the text in the td tag into a list \n",
    "            date_scrape = x.td.text.split()\n",
    "    #         print(date_scrape)\n",
    "            # If the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "            # If not, load 'date' as the 1st element and 'time' as the second\n",
    "            if len(date_scrape) == 1:\n",
    "                time = date_scrape[0]\n",
    "            else:\n",
    "                date = date_scrape[0]\n",
    "                time = date_scrape[1]\n",
    "\n",
    "            parsed_news.append([date, time, x.a.text])\n",
    "\n",
    "\n",
    "    # NLTK VADER for sentiment analysis\n",
    "#     !pip install vaderSentiment\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "    # New words and values\n",
    "    new_words = {\n",
    "        'crushes': 10,\n",
    "        'beats': 5,\n",
    "        'misses': -5,\n",
    "        'trouble': -10,\n",
    "        'falls': -100,\n",
    "    }\n",
    "    # Instantiate the sentiment intensity analyzer with the existing lexicon\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    # Update the lexicon\n",
    "    # nltk.download('stopwords')\n",
    "    vader.lexicon.update(new_words)\n",
    "\n",
    "\n",
    "    # Use these column names\n",
    "    columns = ['date', 'time', 'headline']\n",
    "    # Convert the list of lists into a DataFrame\n",
    "    scored_news = pd.DataFrame(parsed_news,columns=columns)\n",
    "    # Iterate through the headlines and get the polarity scores\n",
    "    scores = [vader.polarity_scores(news[2]) for news in parsed_news]\n",
    "    # Convert the list of dicts into a DataFrame\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    # Join the DataFrames\n",
    "    scored_news = scored_news.join(scores_df)\n",
    "    # Convert the date column from string to datetime\n",
    "    scored_news['date'] = pd.to_datetime(scored_news.date).dt.date\n",
    "\n",
    "    # Count the number of headlines in scored_news (store as integer)\n",
    "    num_news_before = len(scored_news)\n",
    "    # sum(scored_news.duplicated(subset=['headline']))\n",
    "    # Drop duplicates based on ticker and headline\n",
    "    scored_news_clean = scored_news.drop_duplicates(subset = ['headline'])\n",
    "    # Count number of headlines after dropping duplicates\n",
    "    num_news_after = len(scored_news_clean)\n",
    "    # Print before and after numbers to get an idea of how we did \n",
    "#   print(f\"Before we had {num_news_before} headlines, now we have {num_news_after}\")\n",
    "    print(scored_news_clean.head(20))\n",
    "\n",
    "\n",
    "    # Set the index to ticker and date\n",
    "    scored_news_date = scored_news_clean.set_index(['date'])\n",
    "    # Group by mean of compound score of each date\n",
    "    scored_news_date = scored_news_date.groupby('date').mean()\n",
    "    print(scored_news_date['compound'])\n",
    "\n",
    "    current_month = datetime.now().strftime('%B')\n",
    "    TITLE = \"Mean compound sentiment score for \" + Ticker + \" in \" + current_month\n",
    "    return scored_news_date['compound'].plot.bar(title=TITLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ticker = input(\"Ticker of the company?\")\n",
    "SentimentAnalyzer(Ticker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
